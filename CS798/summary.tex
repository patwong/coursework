% <- percent signs are used to comment
%\documentclass[12pt]{article}
\documentclass[11pt,a4paper]{report}
%amsmath is a packaged use for typesetting math
%amsfonts is required for special fonts, e.g. blackboard bold (for denoting real numbers, etc.)
\usepackage{amsmath,amsfonts}

%graphicx is required for images
\usepackage{graphicx}

%used for customzing enumarations
\usepackage{enumerate}

\usepackage[top=0.5in]{geometry}
\usepackage{amssymb,epsfig,color,xspace}

\usepackage{amsthm}
\newcommand\tab[1][1cm]{\hspace*{#1}}
%\renewcommand{\thesubsection}{Problem \arabic{subsection}}

%Note: Many of the packages above have other uses beyond those used in this document
\usepackage{scrextend}
%this marks the beginning of the document. Everything before this is called the Preamble.


%%unnumbered theorem
%source on using newtheoremstyle;
%http://tex.stackexchange.com/questions/12913/customizing-theorem-name
%http://tex.stackexchange.com/questions/234676/new-theorem-style
%a lot more helpful than wikibooks!
\newtheoremstyle{theoremdd}% name of the style to be used
{\topsep}% measure of space to leave above the theorem. E.g.: 3pt
{\topsep}% measure of space to leave below the theorem. E.g.: 3pt
{\itshape}% name of font to use in the body of the theorem
{0pt}% measure of space to indent
{\bfseries}% name of head font
{ ---}% punctuation between head and body
{ }% space after theorem head; " " = normal interword space
{\thmnote{#3}}

\theoremstyle{theoremdd}
\newtheorem{thmd}{Theorem}[section]

%underlined "theorem" for prof lau's claims (sub of theorem)
\newtheoremstyle{claims1}% name of the style to be used
{\topsep}% measure of space to leave above the theorem. E.g.: 3pt
{\topsep}% measure of space to leave below the theorem. E.g.: 3pt
{}% name of font to use in the body of the theorem
{0pt}% measure of space to indent
{\underline}% name of head font
{ }% punctuation between head and body
{.5em}% space after theorem head; " " = normal interword space
{\thmnote{#3}}

\theoremstyle{claims1}
\newtheorem{claimsyay}{Theorem}[section]

%
\newtheoremstyle{proofs1}% name of the style to be used
{\topsep}% measure of space to leave above the theorem. E.g.: 3pt
{\topsep}% measure of space to leave below the theorem. E.g.: 3pt
{}% name of font to use in the body of the theorem
{0pt}% measure of space to indent
{}% name of head font
{ }% punctuation between head and body
{.5em}% space after theorem head; " " = normal interword space
{\thmnote{#3}}

\theoremstyle{proofs1}
\newtheorem{proofwrite}{Proof}[section]

%for dashed list
%source: http://tex.stackexchange.com/questions/62496/itemize-with-a-dash-intstead-of-a-bullet
\def\labelitemi{--}

%source on removing chapter numbering w/o affecting ToC
%http://tex.stackexchange.com/questions/110840/how-to-remove-chapter-numbering-without-removing-it-from-tableofcontents
\newcommand{\mychapter}[2]{
	\setcounter{chapter}{#1}
	\setcounter{section}{0}
	\chapter*{#2}
	\addcontentsline{toc}{chapter}{#2}
}

%to add variables to newenvironment
%[1] is argument, [default] is default arg
%http://tex.stackexchange.com/questions/968/passing-options-to-the-newenvironment
%itemize on same line
%http://tex.stackexchange.com/questions/147710/how-to-start-itemize-on-same-line-as-text
\newenvironment{courseeval}[1][default]
	{#1 \begin{minipage}[t]{\linewidth}\begin{itemize}}
	{\end{itemize}\end{minipage}}



\begin{document}
	
\title{CS 798: Algorithmic Spectral Graph Theory}
\date{Fall 2015}
\author{Lap Chi Lau}
\maketitle

\renewcommand{\abstractname}{A Brief Forward}
\begin{abstract}
	Compiling Professor Lau's handwritten notes on CS798 in LaTeX.
	\newline\newline - Patrick Wong
\end{abstract}

\tableofcontents

\mychapter{1}{Lecture 1: Course Introduction}
I will start with some basic information, and then do an overview of the technical content of the courses, and finally a review of linear algebra for the course.

\section{Course Information}
Objective: learn how to use eigenvalues and eigenvectors to design algorithms and prove theorems. More generally, we will use linear algebraic and continuous techniques to solve combinatorial problems.
\newline\newline
Course page: https://cs.uwaterloo.ca/$\sim$lapchi/cs798
\newline\newline
\begin{courseeval}[Course Evaluation:]
	\setlength\itemsep{0.01em}
	\item Homework 50\%, 2-3 assignments
	\item Project 50\%
\end{courseeval}
%http://tex.stackexchange.com/questions/50269/itemize-without-bullets
\newline\newline
\begin{courseeval}[Project:]
	%http://tex.stackexchange.com/questions/12373/how-to-change-the-space-between-the-itemize-items-in-latex
	\setlength\itemsep{0.01em}
	\item To study one topic in-depth
	\item Some suggested topics in the project page (also served as further references for the course)
	\item encourage to choose a topic close to your research interest
	\item usually survey type, should be related to spectral grpah theory
\end{courseeval}
\newline\newline
\begin{courseeval}[Prerequisites:]
	\setlength\itemsep{0.01em}
	\item[] linear algebra, probability, algorithms, discrete mathematics (graphs)
	\item[] All of them are covered in undergraduate computer science curriculum
	\item[] Could get a good idea about the background of linear algebra in today's review
	\item[] Should like math, as there will be lots of proofs and calculations
	\item[] May be demanding, as definitions and techniques building up (e.g. missing two weeks will be tough)
	\item[] Pace may be fast. Material may be too much
\end{courseeval}
\newline
\newline
References: notes will be provided. No textbook.
\begin{addmargin}[1em]{0em}
	 You can see the lecture notes in my previous offering in 2012 to get a good idea, but there are new topics and some presentations of old topics will be changed.
	 \newline
	 You can also see the notes of similar courses in other universities as linked in the course page. I usually provide more details in the notes than in the lectures.
\end{addmargin}

\section{Course Overview}
There are some recent breakthroughs in using ideas and techniques from spectral graph theory to design better algorithms for graph problems (both better in approximation and faster running time), and also prove new theorems. In this course, we aim to study these new techniques and see the new connections.
\newline
Spectral graph theory is not a new topic.
\newline
About thirty years ago, an important connection was made between the second eigenvalue and graph expansion. This connection made by Cheeger's inequality is useful in different areas:
\begin{itemize}
	\renewcommand\labelitemi{--}
	\item construction of expander graphs, which have various applications in theory and practice
	\item analysis of mixing time in random walks, with many applications in sampling and counting
	\item graph partitioning, and the spectral partitioning algorithm is a widely used heuristic in practice
\end{itemize}
\noindent We will study these in the beginning of the course.
\newline\newline Most of the early results in spectral graph theory are about the second eigenvalue.
\newline In the last few years, researchers have found new connections between higher eigenvalues and graph properties, including the use of the k-th eigenvalue to find a small non-expanding set, to partition the graph into k non-expanding sets, and to do better analysis of the spectral partitioning algorithm and SDP-based approximation algorithms.
\newline We will discuss some of these, but probably not all details.
\newline\newline Then, we will study random walks on graphs, and see the classical connection between the second eigenvalue and the mixing time.
\newline After that, we see some recent results in using random walks to find a small non-expanding set. This problem is closely related to the unique games conjecture, which is about the limits of polynomial time approximation algorithms. Combining ideas from random walks and higher eigenvalues gives the best attempt to go beyond the unique games barrier.
\newline This is about half the course. We will also talk about constructions of expander graphs and small-set expanders if time permits.
\newline\newline Through the study of random walks, we will come across the idea of interpreting the graph as an electrical network, which is used to analyze hitting times of random walks.
\newline This has found surprising applications recently.
\newline One line of research is to use ideas from convex optimization to compute these electrical flows quickly, and then somehow these could be used to compute electrical flows faster!
\newline The other direction is to use these concepts for graph sparsification, where the spectral perspective proved to be the right way to look at this combinatorial problem.
\newline\newline Finally, we study an exciting new technique called the method of interlacing polynomials. This is a new probablistic method showing the existence of good combinatorial objects, but the proofs involve some mathematical concepts like real-stable polynomials. This method is used to make some breakthroughs in constructing expander graphs (Ramanujan graphs) and partitioning into expander graphs (the Kadison-Singer problem).
\newline We will also discuss an amazing application of these ideas in designing approximation algorithms for the asymmetric traveling salesman problem.
\newline The ideas developed in graph sparsification (e.g. barrier argument) is also a key component in the last part of the course.
\newline\newline This overview is still very sketchy. In class, we will elaborate with more precise definition and more details. This is an unusual exception that the lecture has more details than the notes.
\newline\newline Some important topics that will not be covered include SDP-based approximation algorithms, eigenvalues of random graphs, and applications of the spectral methods in machine learning.
\newline\newline The topics are theoretically-oriented. I will try to focus on the underlying techniques that are relevant in broader contexts.
\newline\newline If you are interested in learning more about these topics before the lecture (e.g. previewing the method of interlacing polynomials), the course project page provides further references.

\section{Background on Linear Algebra}
\subsection{Linear transformation, linear independence, nullspace, rank}
Let $A\in R^{mxn}$ be a $n\ x \ n$ matrix, and $x\in R^{n}$ be a column vector (we use $x$ to denote a column vector and $x^T$ to denote a row vector).
\newline One can view $A$ as a matrix or \underline{linear transformation} from n-dimensional vectors to n-dimensional vectors.
\newline A set of vectors $v_1,v_2,...,v_k$ are \underline{linearly independent} if $c_1 v_1 + c_2 v_2 + ... + c_k v_k = 0$ implies $c_1=c_2=...=c_k=0$; otherwise they are \underline{linearly dependent}.
\newline The \underline{nullspace} of $A$ (or the kernel of $A$) is defined as $nullspace(A) = \{x\in R^n \ \vert \ Ax=0 \}$
and its dimension is defined as $null(A)$.
\newline The \underline{range} of $A$ (or the \underline{image} of $A$) is defined as $range(A) = \{Ax \ \vert \ x\in R^n\}$ and its dimension is defined as the \underline{rank} of A, denoted as $rank(A)$.
\newline It is known that $rank(A)+null(A)=$ if $A$ is a $n\ x \ n$ matrix.
\newline It follows from the definition that $rank(A)$ is the number linearly independent columns of $A$.
\subsection{Determinant}
The determinant of $A$, denoted by $det(A)$, is defined recursively as $$det(A)=\sum_{j=1}^{n}(-1)^{i+j}a_{ij}\cdot  det(\tilde{A}_{ij})$$
\indent where $a_{ij}$ is the (i, j)-th entry of $A$ and $\tilde{A}$ is the matrix obtained from $A$ by deleting the $i$-th row and $j$-th column.
\newline We can unfold the recursion and get an equivalent (expansion) definition:
$$ det(A)=\sum_{\sigma\in S^n}^{} sgn(\sigma) \prod_{i=1}^{n}a_{i,\sigma(i)}$$
\indent where $\sigma :[n]\rightarrow [n]$ is a permutation (or a bijection) of the indices, and $sgn(\sigma)=+1$ if $\sigma$ is even and $sgn(\sigma)=-1$ if $\sigma$ is odd. More precisely, $sgn(\sigma)=(-1)^{inv(\sigma)}$ where $ inv(\sigma)=\vert \{(i,j) \ \vert \ i < j \ and \ \sigma (i) > \sigma (j) \} \vert $ is the number of inversions of $\sigma$
\newline\newline A basic property of the determinant is the following:
\[ det\begin{pmatrix}
a_1 \\ \vdots \\ a_{r-1} \\ u+kv \\ a_{r+1} \\ \vdots \\ a_n
\end{pmatrix}
= 
det\begin{pmatrix}
a_1 \\ \vdots \\ a_{r-1} \\ u \\ a_{r+1} \\ \vdots \\ a_n
\end{pmatrix}
+
k\cdot det \begin{pmatrix}
a_1 \\ \vdots \\ a_{r-1} \\ v \\ a_{r+1} \\ \vdots \\ a_n
\end{pmatrix}
\] which follows from the expansion definition of the determinant.
\newline From this it follows that 
\[
det\begin{pmatrix}\vdots \\ ca_i \\ \vdots\end{pmatrix}
= c\cdot det\begin{pmatrix}\vdots \\ a_i \\ \vdots\end{pmatrix}, \ 
det\begin{pmatrix}v \\ v \\ \vdots\end{pmatrix} = 0, \ 
det\begin{pmatrix}\vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \end{pmatrix} = 
-det\begin{pmatrix}\vdots \\ a_j \\ \vdots \\ a_i \\ \vdots \end{pmatrix}, \ 
det\begin{pmatrix}\vdots \\ a_i + ca_j \\ \vdots \\ a_j \\ \vdots \end{pmatrix} = det\begin{pmatrix}\vdots \\ a_i \\ \vdots \\ a_j \\ \vdots \end{pmatrix}
\]
Therefore, we can compute the determinant by reducing $A$ into an upper triangular matrix by elementary row operations, and prove that $det(A)\ne 0$ if and only if $rank(A)=n$ (because $rank(A)=n$ if and only if it can be reduced to an upper triangular matrix where all the diagonal entries are nonzero).
\newline\newline One can also prove that $det(AB)=det(A)det(B)$, as this is true for elementary matrices and one can write $A$ and $B$ as products of elementary matrices if $rank(A)=rank(B)=n$

\subsection{Eigenvalues and Eigenvectors}
A nonzero vector $x$ is an \underline{eigenvector} of $A$ if there exists $\lambda$ such that $Ax=\lambda x$, and the scalar $\lambda$ is called an \underline{eigenvalue} of $A$.
\newline Note that $Ax=\lambda x$ iff $(A-\lambda I)x = 0$ where $I = \begin{pmatrix}1 & & 0 \\ & \ddots & \\ 0 & & 1 \end{pmatrix}$ is the identity matrix.
\newline There is a nonzero vector x satisfying $(A-\lambda I)x = 0$
\begin{align*}
&\iff nullspace(A-\lambda I)\ne \{0\} \\
& \iff rank(A-\lambda I) < n \\
& \iff det(A-\lambda I)=0, 
i.e. \ det\begin{pmatrix}
a_{11}-\lambda & a_{12} & \cdots & a_{1n} \\
a_{21}-\lambda & a_{22}-\lambda & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1}-\lambda & a_{n2} & \cdots & a_{nn}-\lambda
\end{pmatrix}
\end{align*}
By the (expansion) definition of the determinant, $det(A-\lambda I)$ is a polynomial of $\lambda$ of degree $n$.
\newline This is called the \underline{characteristic polynomial} of $A$.
\newline In the last offering of this course, the characteristic polynomials did not play a major role, but they will be very important in the last part of this offering as they will be the key object in the interlacing method. Also, determinants will play an important role in the last part.
\newline\newline One can compute the characteristic polynomial by Gaussian elimination or interpolation.
\newline Any root of this polynomial is an eigenvalue, and any vector in $nullspace(A-\lambda I)$ is an eigenvector.
\newline Geometrically, an eigenvector is a direction that is "fixed" (but can be scaled) by the linear transformation.
\newline So not all matrices have an eigenvector with a real eigenvalue, e.g. the rotation matrix has no direction fixed, and thus no eigenvector with a real eigenvalue.
\newline But we will see that all real symmetric matrices have real eigenvalues, a corollary of the spectral theorem that we will prove.

\subsection{Orthogonality}
To state the spectral theorem, we need one more concept.
\newline Given to n-dimensional vectors 
$u=\begin{pmatrix} u_1 \\ \vdots \\ u_n \end{pmatrix}$ and 
$v=\begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}$, the \underline{inner product} of $u$ and $v$ is defined as $\langle u,v\rangle = \sum_{i=1}^{n}u_i v_i$, and we define the \underline{norm} (or length) of $u$ as $\| u \| = \sqrt{\langle u,u \rangle}$.
\newline Two vectors $u$ and $v$ are \underline{orthogonal} (or perpendicular) if $\langle u,v\rangle = 0$ 
\newline A set $S$ of vectors is orthogonal if $u,v$ are orthogonal for any distinct $u,v\in S$.
\newline A set $S$ of vectors is \underline{orthonormal} if $S$ is orthogonal and $\| u \| = 1$ for every $u\in S$.
\newline A set $S$ of vectors is a \underline{basis} for a vector space $V$ if $S$ is linearly independent and every vector of $V$ is a linear combination of the vectors in $S$.
\newline Given any basis, we can construct an orthogonal basis by the Gram-Schmidt orthogonalization process. For example, given two vectors $v_1 , v_2$, we can construct ${v_2}'$ by subtracting the projection of $v_2$ onto $v_1$. **(drawing in handwritten notes)
\newline Given a basis $S = \{w_1 , w_2 ,\cdots, w_3\}$, define $S' = \{v_1 , v_2 ,\cdots, v_n\}$ where
\[ v_k = w_k - \sum_{j=1}^{k-1} \frac{\langle w_k , v_j \rangle}{\| v_j\|^2} v_j , \ for \ 2\leq k \leq n
\]
Then $S'$ is an orthogonal basis.
\newline Then $S'' = \{ \frac{v_1}{\|v_1\|}, \cdots , \frac{v_n}{\|v_n\|}\}$ is an orthonormal basis.
\newline Having an orthonormal basis is easy for computation, e.g. if $A$ is a matrix whose columns form an orthonormal basis, then $A^T A = I$ and hence $A^{-1} = A^T$ where $A^{-1}$ denotes the inverse of $A$.

\subsection{Spectral theorem for real symmetric matrices}
(follow the the presentation in Godsil and Royle)

\begin{thmd}[Spectral Theorem for real symmetric matrices]
	Let A be a real symmetric n-x-n matrix. Then there is an orthonormal basis of $\mathbb{R}^n$ consisting of eigenvectors of A, and the corresponding eigenvalues are real numbers.
\end{thmd}
\noindent First we prove that there is a real eigenvalue.

\begin{claimsyay}[\underline{Claim 1}]
	Let A be a real symmetric matrix. \newline\indent\indent If u and v are eigenvectors with different eigenvalues, then u and v are orthogonal.
\end{claimsyay}

\begin{proofwrite}[\underline{Proof}]
	Let $Au=\alpha u$ and $Av = \beta v$, where $\alpha \ne \beta$.
\end{proofwrite}


\section{Applications of the spectral theorem}
\subsection{Power of matrices}

\mychapter{2}{Lecture 2: Graph Spectrum}
forward

\section{Adjacency Matrix}
this

\end{document}
